{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "Batch = namedtuple('Batch', 'features labels')\n",
    "\n",
    "class DataLoader(object):\n",
    "    def __init__(self, data, batch_size=8):\n",
    "        assert isinstance(data, str) and len(data) > 1\n",
    "        assert 0 < batch_size < len(data)\n",
    "        self._data = self._preprocess_data(data)\n",
    "        self._batch_size = batch_size\n",
    "        self.curr_pos = 0\n",
    "        \n",
    "        all_chars = sorted(set(self._data))\n",
    "        self._char_to_index = {ch:idx for idx, ch in enumerate(all_chars)}\n",
    "        self._index_to_char = {idx:ch for ch, idx in self._char_to_index.items()}\n",
    "    \n",
    "    @property\n",
    "    def num_examples(self):\n",
    "        return len(self._data)-1\n",
    "    \n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self._char_to_index)\n",
    "    \n",
    "    @property\n",
    "    def vocab(self):\n",
    "        return set(self._char_to_index.keys())\n",
    "    \n",
    "    def next_batch(self):\n",
    "        data = self._data[self.curr_pos:self.curr_pos+self._batch_size+1]\n",
    "        self.curr_pos += self._batch_size\n",
    "        if self.curr_pos >= self.num_examples:\n",
    "            self.curr_pos = 0\n",
    "        data = self._one_hot(data)\n",
    "        return Batch(data[:-1, :], data[1:, :])\n",
    "    \n",
    "    def _one_hot(self, s):\n",
    "        one_hot = np.zeros((len(s), self.vocab_size))\n",
    "        idx = [self._char_to_index[ch] for ch in s]\n",
    "        one_hot[np.arange(len(s)), idx] = 1\n",
    "        return one_hot\n",
    "    \n",
    "    def decode(self, one_hot):\n",
    "        if len(one_hot.shape) == 1:\n",
    "            one_hot = one_hot.reshape(1, -1)\n",
    "        return ''.join(map(self.decode_char, one_hot))\n",
    "    \n",
    "    def decode_char(self, one_hot):\n",
    "        idx = np.argmax(one_hot)\n",
    "        return self._index_to_char[idx]\n",
    "    \n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            yield next(self)\n",
    "            if self.curr_pos == 0:\n",
    "                break\n",
    "    \n",
    "    def __next__(self):\n",
    "        return self.next_batch()\n",
    "    \n",
    "    def _preprocess_data(self, data):\n",
    "        return data.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    with open('data/input.txt', 'r') as fp:\n",
    "        return fp.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def forward(self, *args, **kwrgs):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def backward(self, *args, **kwrgs):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def __call__(self, *args, **kwrgs):\n",
    "        return self.forward(*args, **kwrgs)\n",
    "\n",
    "class Affine(Layer):\n",
    "    def forward(self, x, w, b):\n",
    "        self.cache = (x, w, b)\n",
    "        return x.dot(w) + b\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        x, w, b = self.cache\n",
    "        db = dout.sum(axis=0)\n",
    "        dw = x.T.dot(dout)\n",
    "        dx = dout.dot(w.T)\n",
    "        return dx, dw, db\n",
    "\n",
    "class Tanh(Layer):\n",
    "    def forward(self, x):\n",
    "        out = np.tanh(x)\n",
    "        self.cache = out\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        out = self.cache\n",
    "        return (1-out*out)*dout\n",
    "\n",
    "class CrossEntropy(Layer):\n",
    "    def forward(self, logits, target):\n",
    "        target = np.argmax(target, axis=1)\n",
    "        logits = logits.copy()\n",
    "        logits -= np.max(logits, axis=1)\n",
    "        unnormalized_probs = np.exp(logits)\n",
    "        probs = unnormalized_probs / np.sum(unnormalized_probs, axis=1, keepdims=True)\n",
    "        correct_class_probs = probs[np.arange(len(logits)), target]\n",
    "        self.cache = (probs, target)\n",
    "        return np.mean(-np.log(correct_class_probs))\n",
    "    \n",
    "    def backward(self):\n",
    "        probs, target = self.cache\n",
    "        dlogits = probs.copy()\n",
    "        dlogits[np.arange(len(dlogits)), target] -= 1\n",
    "        return dlogits / dlogits.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(Layer):\n",
    "    def forward(self, hidden_state, features, labels, params):\n",
    "        self.params = params\n",
    "        i2h, i2h_b = params['i2h'], params['i2h_b']\n",
    "        h2h, h2h_b = params['h2h'], params['h2h_b']\n",
    "        h2o, h2o_b = params['h2o'], params['h2o_b']\n",
    "        num_chars = features.shape[0]\n",
    "        self.affine_hidden = []\n",
    "        self.affine_input = []\n",
    "        self.tanh = []\n",
    "        self.affine_output = []\n",
    "        self.loss = []\n",
    "        loss = 0\n",
    "        for i in range(num_chars):\n",
    "            x = np.expand_dims(features[i], 0)\n",
    "            y = np.expand_dims(labels[i], 0)\n",
    "            affine_hidden = Affine()\n",
    "            affine_input = Affine()\n",
    "            affine_output = Affine()\n",
    "            cross_entropy = CrossEntropy()\n",
    "            hidden_raw = affine_input(x, i2h, i2h_b) + affine_hidden(hidden_state, h2h, h2h_b)\n",
    "            tanh = Tanh()\n",
    "            hidden_state = tanh(hidden_raw)\n",
    "            logits = affine_output(hidden_state, h2o, h2o_b)\n",
    "            loss += cross_entropy(logits, y)\n",
    "\n",
    "            self.affine_hidden.append(affine_hidden)\n",
    "            self.affine_input.append(affine_input)\n",
    "            self.tanh.append(tanh)\n",
    "            self.affine_output.append(affine_output)\n",
    "            self.loss.append(cross_entropy)\n",
    "        return loss / len(features), hidden_state\n",
    "    \n",
    "    def backward(self):\n",
    "        next_dhidden = 0\n",
    "        grads = {k: np.zeros_like(v) for k, v in self.params.items()}\n",
    "        while len(self.affine_hidden) > 0:\n",
    "            loss = self.loss.pop()\n",
    "            dlogits = loss.backward()\n",
    "            \n",
    "            affine_output = self.affine_output.pop()\n",
    "            dhidden, dh2o, dh2o_b = affine_output.backward(dlogits)\n",
    "            dhidden = dhidden + next_dhidden\n",
    "            grads['h2o'] += dh2o\n",
    "            grads['h2o_b'] += dh2o_b\n",
    "            \n",
    "            tanh = self.tanh.pop()\n",
    "            dhidden_raw = tanh.backward(dhidden)\n",
    "            \n",
    "            affine_input = self.affine_input.pop()\n",
    "            dx, di2h, di2h_b = affine_input.backward(dhidden_raw)\n",
    "            grads['i2h'] += di2h\n",
    "            grads['i2h_b'] += di2h_b\n",
    "            \n",
    "            affine_hidden = self.affine_hidden.pop()\n",
    "            dhidden_prev, dh2h, dh2h_b = affine_hidden.backward(dhidden_raw)\n",
    "            grads['h2h'] += dh2h\n",
    "            grads['h2h_b'] += dh2h_b\n",
    "            \n",
    "            next_dhidden = dhidden_prev\n",
    "        \n",
    "        return grads\n",
    "            \n",
    "def init_params(vocab_size, hidden_size, std=0.01):\n",
    "    init_weights = lambda size: np.random.randn(*size)*std\n",
    "    init_bias = lambda num_outputs: np.zeros((num_outputs,))\n",
    "    params = {\n",
    "        'i2h': init_weights((vocab_size, hidden_size)),\n",
    "        'h2h': init_weights((hidden_size, hidden_size)),\n",
    "        'h2o': init_weights((hidden_size, vocab_size)),\n",
    "        'i2h_b': init_bias(hidden_size),\n",
    "        'h2h_b': init_bias(hidden_size),\n",
    "        'h2o_b': init_bias(vocab_size)\n",
    "    }\n",
    "    return params\n",
    "\n",
    "\n",
    "def sample(rnn, hidden_state, start_char):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def step(self, *args, **kwrgs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class Adam(Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, beta1=.9, beta2=.99, eps=1e-3):\n",
    "        self.params = params\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = eps\n",
    "        self.dx = {k: np.zeros_like(v) for k, v in params.items()}\n",
    "        self.dx2 = {k: np.zeros_like(v) for k, v in params.items()}\n",
    "    \n",
    "    def step(self, grads):\n",
    "        for param_name, grad_value in grads.items():\n",
    "            param_value = self.params[param_name]\n",
    "            m = self.beta1*self.dx[param_name] + (1-self.beta1)*grad_value\n",
    "            v = self.beta2*self.dx2[param_name] + (1-self.beta2)*(grad_value*grad_value)\n",
    "            self.dx[param_name] = m\n",
    "            self.dx2[param_name] = v\n",
    "            self.params[param_name] = param_value - self.lr*m / np.sqrt(v + self.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it: 0, loss: 3.2576426012210336\n",
      "it: 100, loss: 3.2298292198352443\n",
      "it: 200, loss: 3.251130540862038\n",
      "it: 300, loss: 3.196794440759586\n",
      "it: 400, loss: 3.1269258917975713\n",
      "it: 500, loss: 2.94407508107704\n",
      "it: 600, loss: 2.1365516068773025\n",
      "it: 700, loss: 1.3182259383038677\n",
      "it: 800, loss: 0.9968493556733904\n",
      "it: 900, loss: 0.7923963624947317\n",
      "it: 1000, loss: 0.6251716142606709\n",
      "it: 1100, loss: 0.4958127794885815\n",
      "it: 1200, loss: 0.38073855354151565\n",
      "it: 1300, loss: 0.287894490211315\n",
      "it: 1400, loss: 0.22909915270624484\n",
      "it: 1500, loss: 0.1823449287386685\n",
      "it: 1600, loss: 0.12241460449523407\n",
      "it: 1700, loss: 0.07907456837143093\n",
      "it: 1800, loss: 0.06964367933669009\n",
      "it: 1900, loss: 0.05054010390215904\n",
      "it: 2000, loss: 0.037459008791289626\n",
      "it: 2100, loss: 0.03384241858416666\n",
      "it: 2200, loss: 0.020679670625307595\n",
      "it: 2300, loss: 0.02126603626774248\n",
      "it: 2400, loss: 0.014916389786293569\n",
      "it: 2500, loss: 0.013637400660654627\n",
      "it: 2600, loss: 0.01273527926959669\n",
      "it: 2700, loss: 0.012141979462648896\n",
      "it: 2800, loss: 0.011440532885513128\n",
      "it: 2900, loss: 0.011027980917487374\n",
      "it: 3000, loss: 0.010996829539407515\n",
      "it: 3100, loss: 0.010891445979809265\n",
      "it: 3200, loss: 0.011091639070834132\n",
      "it: 3300, loss: 0.010675125380125206\n",
      "it: 3400, loss: 0.00914725187244996\n",
      "it: 3500, loss: 0.00820130314349254\n",
      "it: 3600, loss: 0.00821371470627156\n",
      "it: 3700, loss: 0.008183788573734184\n",
      "it: 3800, loss: 0.007736478664576921\n",
      "it: 3900, loss: 0.007232345914487059\n",
      "it: 4000, loss: 0.006680906540953223\n",
      "it: 4100, loss: 0.006146765089609871\n",
      "it: 4200, loss: 0.005430290445296677\n",
      "it: 4300, loss: 0.0047084313229790375\n",
      "it: 4400, loss: 0.004232350847815289\n",
      "it: 4500, loss: 0.003914832539514478\n",
      "it: 4600, loss: 0.003733934487334388\n",
      "it: 4700, loss: 0.0036733753305969613\n",
      "it: 4800, loss: 0.003660319354730955\n",
      "it: 4900, loss: 0.0036777208229023762\n",
      "it: 5000, loss: 0.003846627927032545\n",
      "it: 5100, loss: 0.004543976324927047\n",
      "it: 5200, loss: 0.005457493742073277\n",
      "it: 5300, loss: 0.10146972734296748\n",
      "it: 5400, loss: 0.022482667727823525\n",
      "it: 5500, loss: 0.006907467198604952\n",
      "it: 5600, loss: 0.006266126525030592\n",
      "it: 5700, loss: 0.005620144972094181\n",
      "it: 5800, loss: 0.005053971132934203\n",
      "it: 5900, loss: 0.012207768978435998\n",
      "it: 6000, loss: 0.012861639577152159\n",
      "it: 6100, loss: 0.00835431680944634\n",
      "it: 6200, loss: 0.0076109845956891225\n",
      "it: 6300, loss: 0.004679776898384179\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-266-016c03552140>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mit\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'it: {}, loss: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mit\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-229-5e0e479dc904>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0maffine_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maffine_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdi2h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdi2h_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maffine_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdhidden_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'i2h'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdi2h\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'i2h_b'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdi2h_b\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data = read_data()\n",
    "data_loader = DataLoader(data[:100], batch_size=5)\n",
    "\n",
    "hidden_size, vocab_size = 100, data_loader.vocab_size\n",
    "rnn = RNN()\n",
    "params = init_params(vocab_size=vocab_size, hidden_size=hidden_size)\n",
    "hidden_state = np.zeros((1, hidden_size))\n",
    "rnn.forward(hidden_state, *next(data_loader), params)\n",
    "optimizer = Adam(params, lr=1e-3)\n",
    "\n",
    "num_epochs = 10000\n",
    "it = 0\n",
    "for epoch in range(num_epochs):\n",
    "    hidden_state = np.zeros((1, hidden_size))\n",
    "    losses = []\n",
    "    for x, y in data_loader:\n",
    "        loss, hidden_state = rnn(hidden_state, x, y, params)\n",
    "        losses.append(loss)\n",
    "        if it % 100 == 0:\n",
    "            print('it: {}, loss: {}'.format(it, loss))\n",
    "        grads = rnn.backward()\n",
    "        optimizer.step(grads)\n",
    "        it += 1\n",
    "    #print('epoch: {}, loss: {}'.format(epoch, np.mean(losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
