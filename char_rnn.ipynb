{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader(object):\n",
    "    def __init__(self, data, batch_size=8):\n",
    "        assert isinstance(data, str) and len(data) > 1\n",
    "        assert 0 < batch_size < len(data)\n",
    "        self._data = self._preprocess_data(data)\n",
    "        self._batch_size = batch_size\n",
    "        self.curr_pos = 0\n",
    "        \n",
    "        all_chars = sorted(set(self._data))\n",
    "        self._char_to_index = {ch:idx for idx, ch in enumerate(all_chars)}\n",
    "        self._index_to_char = {idx:ch for ch, idx in self._char_to_index.items()}\n",
    "    \n",
    "    @property\n",
    "    def num_examples(self):\n",
    "        return len(self._data)-1\n",
    "    \n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self._char_to_index)\n",
    "    \n",
    "    @property\n",
    "    def vocab(self):\n",
    "        return set(self._char_to_index.keys())\n",
    "    \n",
    "    def next_batch(self):\n",
    "        data = self._data[self.curr_pos:self.curr_pos+self._batch_size+1]\n",
    "        self.curr_pos += self._batch_size\n",
    "        if self.curr_pos >= self.num_examples:\n",
    "            self.curr_pos = 0\n",
    "        data = self._one_hot(data)\n",
    "        return data[:-1, :], data[1:, :]\n",
    "    \n",
    "    def _one_hot(self, s):\n",
    "        one_hot = np.zeros((len(s), self.vocab_size))\n",
    "        idx = [self._char_to_index[ch] for ch in s]\n",
    "        one_hot[np.arange(len(s)), idx] = 1\n",
    "        return one_hot\n",
    "    \n",
    "    def decode(self, one_hot):\n",
    "        if len(one_hot.shape) == 1:\n",
    "            one_hot = one_hot.reshape(1, -1)\n",
    "        return ''.join(map(self.decode_char, one_hot))\n",
    "    \n",
    "    def decode_char(self, one_hot):\n",
    "        idx = np.argmax(one_hot)\n",
    "        return self._index_to_char[idx]\n",
    "    \n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            yield next(self)\n",
    "            if self.curr_pos == 0:\n",
    "                break\n",
    "    \n",
    "    def __next__(self):\n",
    "        return self.next_batch()\n",
    "    \n",
    "    def _preprocess_data(self, data):\n",
    "        return data.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    with open('data/input.txt', 'r') as fp:\n",
    "        return fp.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def forward(self, *args, **kwrgs):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def backward(self, *args, **kwrgs):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def __call__(self, *args, **kwrgs):\n",
    "        return self.forward(*args, **kwrgs)\n",
    "\n",
    "    \n",
    "class Affine(Layer):\n",
    "    def forward(self, x, w, b):\n",
    "        self.cache = (x, w, b)\n",
    "        return x.dot(w) + b\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        x, w, b = self.cache\n",
    "        db = dout.sum(axis=0)\n",
    "        dw = x.T.dot(dout)\n",
    "        dx = dout.dot(w.T)\n",
    "        return dx, dw, db\n",
    "\n",
    "    \n",
    "class Tanh(Layer):\n",
    "    def forward(self, x):\n",
    "        out = np.tanh(x)\n",
    "        self.cache = out\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        out = self.cache\n",
    "        return (1-out*out)*dout\n",
    "\n",
    "    \n",
    "class CrossEntropy(Layer):\n",
    "    def forward(self, logits, target):\n",
    "        if len(logits.shape) == 1:\n",
    "            logits = np.expand_dims(logits, 0)\n",
    "        if len(target.shape) == 1:\n",
    "            target = np.expand_dims(target, 0)\n",
    "        target = np.argmax(target, axis=1)\n",
    "        logits = logits.copy()\n",
    "        logits -= np.max(logits, axis=1)\n",
    "        unnormalized_probs = np.exp(logits)\n",
    "        probs = unnormalized_probs / np.sum(unnormalized_probs, axis=1, keepdims=True)\n",
    "        correct_class_probs = probs[np.arange(len(logits)), target]\n",
    "        self.cache = (probs, target)\n",
    "        return np.mean(-np.log(correct_class_probs))\n",
    "    \n",
    "    def backward(self):\n",
    "        probs, target = self.cache\n",
    "        dlogits = probs.copy()\n",
    "        dlogits[np.arange(len(dlogits)), target] -= 1\n",
    "        return dlogits / dlogits.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCell(Layer):\n",
    "    def forward(self, x, hidden_state_prev, params):\n",
    "        assert len(x.shape) == 2\n",
    "        affine_hidden, affine_input, affine_output = Affine(), Affine(), Affine()\n",
    "        tanh = Tanh()\n",
    "        hidden_state_raw = affine_hidden(hidden_state_prev, params['h2h'], params['h2h_b'])\n",
    "        hidden_state_raw += affine_input(x, params['i2h'], params['i2h_b'])\n",
    "        hidden_state = tanh(hidden_state_raw)\n",
    "        logits = affine_output(hidden_state, params['h2o'], params['h2o_b'])\n",
    "        self.cache = (affine_hidden, affine_input, affine_output, tanh, params)\n",
    "        return hidden_state, logits\n",
    "    \n",
    "    def backward(self, dnext_hidden_state, dlogits):\n",
    "        affine_hidden, affine_input, affine_output, tanh, params = self.cache\n",
    "        dparams = {}\n",
    "        dhidden_state, dparams['h2o'], dparams['h2o_b'] = affine_output.backward(dlogits)\n",
    "        dhidden_state = dhidden_state + dnext_hidden_state\n",
    "        dhidden_state_raw = tanh.backward(dhidden_state)\n",
    "        dhidden_state_prev, dparams['h2h'], dparams['h2h_b'] = affine_hidden.backward(dhidden_state_raw)\n",
    "        dx, dparams['i2h'], dparams['i2h_b'] = affine_input.backward(dhidden_state_raw)\n",
    "        return dx, dhidden_state_prev, dparams\n",
    "\n",
    "\n",
    "class RNN(Layer):\n",
    "    def forward(self, hidden_state, x, params):\n",
    "        num_inputs = len(x)\n",
    "        logits = []\n",
    "        self.cache = []\n",
    "        for i in range(num_inputs):\n",
    "            rnn_cell = RNNCell()\n",
    "            hidden_state, _logits = rnn_cell(np.expand_dims(x[i], 0), hidden_state, params)\n",
    "            logits.append(_logits)\n",
    "            self.cache.append(rnn_cell)\n",
    "        self.cache = (self.cache, params)\n",
    "        return hidden_state, logits\n",
    "    \n",
    "    def backward(self, dlogits):\n",
    "        rnn_cells, params = self.cache\n",
    "        dparams = {k: np.zeros_like(v) for k, v in params.items()}\n",
    "        dnext_hidden_state = 0\n",
    "        while len(rnn_cells) > 0:\n",
    "            rnn_cell = rnn_cells.pop()\n",
    "            _, dnext_hidden_state, _dparams = rnn_cell.backward(dnext_hidden_state, dlogits.pop())\n",
    "            for param_name, grad_value in _dparams.items():\n",
    "                dparams[param_name] += grad_value\n",
    "        return dparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rnn_training_step(rnn, hidden_state, x, y, params):\n",
    "    hidden_state, logits = rnn(hidden_state, x, params)\n",
    "    dlogits = []\n",
    "    loss = 0\n",
    "    for i, l in enumerate(logits):\n",
    "        criterion = CrossEntropy()\n",
    "        loss += criterion(l, y[i])\n",
    "        dlogits.append(criterion.backward())\n",
    "    loss /= len(x)\n",
    "    dparams = rnn.backward(dlogits)\n",
    "    return loss, hidden_state, dparams\n",
    "\n",
    "\n",
    "def sample(rnn, hidden_state, input, params, n=100):\n",
    "    one_hot = []\n",
    "    while n > 0:\n",
    "        if len(input.shape) == 1:\n",
    "            input = np.expand_dims(input, 0)\n",
    "        hidden_state, logits = rnn(hidden_state, input, params)\n",
    "        logits = logits[0].squeeze()\n",
    "        probs = logits_to_probs(logits)\n",
    "        idx = np.random.choice(len(logits), p=probs)\n",
    "        one_hot_char = np.zeros_like(logits)\n",
    "        one_hot_char[idx] = 1\n",
    "        one_hot.append(one_hot_char)\n",
    "        input = one_hot_char\n",
    "        n -= 1\n",
    "    return np.asarray(one_hot)\n",
    "\n",
    "\n",
    "def logits_to_probs(logits):\n",
    "    logits = logits.copy()\n",
    "    logits -= np.max(logits)\n",
    "    unnormalized_probs = np.exp(logits)\n",
    "    return unnormalized_probs / np.sum(unnormalized_probs)\n",
    "\n",
    "\n",
    "def init_params(vocab_size, hidden_size, std=0.01):\n",
    "    init_weights = lambda size: np.random.randn(*size)*std\n",
    "    init_bias = lambda num_outputs: np.zeros((num_outputs,))\n",
    "    params = {\n",
    "        'i2h': init_weights((vocab_size, hidden_size)),\n",
    "        'h2h': init_weights((hidden_size, hidden_size)),\n",
    "        'h2o': init_weights((hidden_size, vocab_size)),\n",
    "        'i2h_b': init_bias(hidden_size),\n",
    "        'h2h_b': init_bias(hidden_size),\n",
    "        'h2o_b': init_bias(vocab_size)\n",
    "    }\n",
    "    return params\n",
    "\n",
    "sample(rnn, hidden_state, x[0], params, n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def step(self, *args, **kwrgs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class Adam(Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, beta1=.9, beta2=.99, eps=1e-8):\n",
    "        self.params = params\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = eps\n",
    "        self.dx = {k: np.zeros_like(v) for k, v in params.items()}\n",
    "        self.dx2 = {k: np.zeros_like(v) for k, v in params.items()}\n",
    "    \n",
    "    def step(self, grads):\n",
    "        for param_name, grad_value in grads.items():\n",
    "            param_value = self.params[param_name]\n",
    "            m = self.beta1*self.dx[param_name] + (1-self.beta1)*grad_value\n",
    "            v = self.beta2*self.dx2[param_name] + (1-self.beta2)*(grad_value*grad_value)\n",
    "            self.dx[param_name] = m\n",
    "            self.dx2[param_name] = v\n",
    "            self.params[param_name] = param_value - self.lr*m / np.sqrt(v + self.eps)\n",
    "        return self.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wkdcwaapkdmmbtmwrpighdgopgrka!gh wrowhnfcgffiwgre!tkmwwhaiff\n",
      "it: 0, loss: 3.0910644653429156\n",
      "it: 1000, loss: 0.08377164816841934\n",
      "it: 2000, loss: 0.0006755729377005408\n",
      "it: 3000, loss: 2.1757301470474312e-05\n",
      "it: 4000, loss: 6.479699640840998e-06\n",
      "it: 5000, loss: 3.9689606252081366e-06\n",
      "it: 6000, loss: 3.123754695730528e-06\n",
      "it: 7000, loss: 3.0324899583466215e-06\n",
      "it: 8000, loss: 2.648846852522479e-06\n",
      "it: 9000, loss: 1.3871291031618162e-06\n",
      " work at deep mind how are you doing bold fuck!ri deep mind \n",
      "it: 10000, loss: 1.242838972457879e-06\n",
      "it: 11000, loss: 1.6113929055848675e-06\n",
      "it: 12000, loss: 2.06054816064e-06\n",
      "it: 13000, loss: 1.5422487566677646e-06\n",
      "it: 14000, loss: 1.1692492915650097e-06\n",
      "it: 15000, loss: 7.916838854739989e-07\n",
      "it: 16000, loss: 7.58351926236691e-07\n",
      "it: 17000, loss: 7.635637944135057e-07\n",
      "it: 18000, loss: 8.804640845422465e-07\n",
      "it: 19000, loss: 8.918848269927499e-07\n",
      " at deep mind how are you doing bold fuck!rt deep mind how a\n",
      "it: 20000, loss: 5.186986966812195e-07\n",
      "it: 21000, loss: 4.95989236628222e-07\n",
      "it: 22000, loss: 6.908760261349605e-07\n",
      "it: 23000, loss: 9.530037970333091e-07\n",
      "it: 24000, loss: 7.508622637419948e-07\n",
      "it: 25000, loss: 5.948754693913954e-07\n",
      "it: 26000, loss: 4.1544242193386035e-07\n",
      "it: 27000, loss: 4.1652088576584114e-07\n",
      "it: 28000, loss: 4.320298174484292e-07\n",
      "it: 29000, loss: 5.116983134274782e-07\n",
      "eep mind how are you doing bold fuck!rt deep mind how are yo\n",
      "it: 30000, loss: 5.343067592218171e-07\n",
      "it: 31000, loss: 3.179084629473925e-07\n",
      "it: 32000, loss: 3.0749898195885837e-07\n",
      "it: 33000, loss: 4.364989152947625e-07\n",
      "it: 34000, loss: 6.17697319755619e-07\n",
      "it: 35000, loss: 4.94569658729671e-07\n",
      "it: 36000, loss: 3.977849951567888e-07\n",
      "it: 37000, loss: 2.804754417295505e-07\n",
      "it: 38000, loss: 2.8648218948424377e-07\n",
      "it: 39000, loss: 3.004979986108642e-07\n",
      "ind how are you doing bold fuck!rt deep mind how are you doi\n",
      "it: 40000, loss: 3.5977120916390865e-07\n",
      "it: 41000, loss: 3.8082434459683876e-07\n",
      "it: 42000, loss: 2.2885729157117387e-07\n",
      "it: 43000, loss: 2.221058051210235e-07\n",
      "it: 44000, loss: 3.179674535337697e-07\n",
      "it: 45000, loss: 4.5621229287892986e-07\n",
      "it: 46000, loss: 3.6812364238416165e-07\n",
      "it: 47000, loss: 2.98382526852221e-07\n",
      "it: 48000, loss: 2.1130057267056354e-07\n",
      "it: 49000, loss: 2.180820681089985e-07\n",
      "ow are you doing bold fuck!rt deep mind how are you doing bo\n",
      "it: 50000, loss: 2.30068529043362e-07\n",
      "it: 51000, loss: 2.7705030107399737e-07\n",
      "it: 52000, loss: 2.955963347621519e-07\n",
      "it: 53000, loss: 1.7863511911442634e-07\n",
      "it: 54000, loss: 1.7352222905820715e-07\n",
      "it: 55000, loss: 2.495787429442006e-07\n",
      "it: 56000, loss: 3.613408921491399e-07\n",
      "it: 57000, loss: 2.9289360316564774e-07\n",
      "it: 58000, loss: 2.385350591219138e-07\n",
      "it: 59000, loss: 1.6931376463762935e-07\n",
      "e you doing bold fuck!rt deep mind how are you doing bold fu\n",
      "it: 60000, loss: 1.759328271265878e-07\n",
      "it: 61000, loss: 1.862376740872088e-07\n",
      "it: 62000, loss: 2.2507879659821009e-07\n",
      "it: 63000, loss: 2.4141231734323525e-07\n",
      "it: 64000, loss: 1.4641395792786245e-07\n",
      "it: 65000, loss: 1.4221717256555506e-07\n",
      "it: 66000, loss: 2.0514054401065976e-07\n",
      "it: 67000, loss: 2.9896459581113666e-07\n",
      "it: 68000, loss: 2.430443776821962e-07\n",
      "it: 69000, loss: 1.9857941667895185e-07\n",
      " doing bold fuck!rt deep mind how are you doing bold fuck!rt\n",
      "it: 70000, loss: 1.4114896001074742e-07\n",
      "it: 71000, loss: 1.4737270002835197e-07\n",
      "it: 72000, loss: 1.563515166379744e-07\n",
      "it: 73000, loss: 1.8942396301658749e-07\n",
      "it: 74000, loss: 2.039407995149854e-07\n",
      "it: 75000, loss: 1.2399672811063427e-07\n",
      "it: 76000, loss: 1.2038723300826869e-07\n",
      "it: 77000, loss: 1.739792233236764e-07\n",
      "it: 78000, loss: 2.548534524563556e-07\n",
      "it: 79000, loss: 2.076053839645685e-07\n",
      "g bold fuck!rt deep mind how are you doing bold fuck!rt deep\n",
      "it: 80000, loss: 1.700254080037654e-07\n",
      "it: 81000, loss: 1.2095976226266998e-07\n",
      "it: 82000, loss: 1.2675070811704425e-07\n",
      "it: 83000, loss: 1.3467901143535797e-07\n",
      "it: 84000, loss: 1.6345719370795013e-07\n",
      "it: 85000, loss: 1.7649144448271421e-07\n",
      "it: 86000, loss: 1.075049973210659e-07\n",
      "it: 87000, loss: 1.043077680418009e-07\n",
      "it: 88000, loss: 1.509355974445652e-07\n",
      "it: 89000, loss: 2.2202211214653533e-07\n",
      "d fuck!rt deep mind how are you doing bold fuck!rt deep mind\n",
      "it: 90000, loss: 1.811278771916212e-07\n",
      "it: 91000, loss: 1.4860956258571302e-07\n",
      "it: 92000, loss: 1.0578614178884905e-07\n",
      "it: 93000, loss: 1.1116577815735681e-07\n",
      "it: 94000, loss: 1.1824894036853997e-07\n",
      "it: 95000, loss: 1.4370932360058375e-07\n",
      "it: 96000, loss: 1.5552270148168038e-07\n",
      "it: 97000, loss: 9.486662535597167e-08\n",
      "it: 98000, loss: 9.197766970257417e-08\n",
      "it: 99000, loss: 1.3321338535777153e-07\n",
      "k!rt deep mind how are you doing bold fuck!rt deep mind how \n",
      "it: 100000, loss: 1.96641763151192e-07\n",
      "it: 101000, loss: 1.6060047016657057e-07\n",
      "it: 102000, loss: 1.3195696173171072e-07\n",
      "it: 103000, loss: 9.396994529995197e-08\n",
      "it: 104000, loss: 9.897602164232453e-08\n",
      "it: 105000, loss: 1.0536782218187848e-07\n",
      "it: 106000, loss: 1.2818949193206626e-07\n",
      "it: 107000, loss: 1.3898488424174503e-07\n",
      "it: 108000, loss: 8.48741963931953e-08\n",
      "it: 109000, loss: 8.222652141323262e-08\n",
      " will work at deep mind how are you doing bold fuck!rt deep \n",
      "it: 110000, loss: 1.1916643624241758e-07\n",
      "it: 111000, loss: 1.7643890503504616e-07\n",
      "it: 112000, loss: 1.4422395464398356e-07\n",
      "it: 113000, loss: 1.1864022603717875e-07\n",
      "it: 114000, loss: 8.451052726862913e-08\n",
      "it: 115000, loss: 8.918269405939308e-08\n",
      "it: 116000, loss: 9.500004860493029e-08\n",
      "it: 117000, loss: 1.1567390201442065e-07\n",
      "it: 118000, loss: 1.2560970891924552e-07\n",
      "it: 119000, loss: 7.67767109685281e-08\n",
      " work at deep mind how are you doing bold fuck!rt deep mind \n",
      "it: 120000, loss: 7.43244226890844e-08\n",
      "it: 121000, loss: 1.0776302700627744e-07\n",
      "it: 122000, loss: 1.5997879072316397e-07\n",
      "it: 123000, loss: 1.308574359860569e-07\n",
      "it: 124000, loss: 1.0775002923206185e-07\n",
      "it: 125000, loss: 7.676846609419229e-08\n",
      "it: 126000, loss: 8.114350361151991e-08\n",
      "it: 127000, loss: 8.64768602648419e-08\n",
      "it: 128000, loss: 1.0536904060968367e-07\n",
      "it: 129000, loss: 1.1457045345589111e-07\n",
      " at deep mind how are you doing bold fuck!rt deep mind how a\n",
      "it: 130000, loss: 7.008262042841286e-08\n",
      "it: 131000, loss: 6.779273825620029e-08\n",
      "it: 132000, loss: 9.832391318232501e-08\n",
      "it: 133000, loss: 1.463114819599987e-07\n",
      "it: 134000, loss: 1.1974263431456528e-07\n",
      "it: 135000, loss: 9.86797298962585e-08\n",
      "it: 136000, loss: 7.031617504371605e-08\n",
      "it: 137000, loss: 7.442668358210689e-08\n",
      "it: 138000, loss: 7.93472048183869e-08\n",
      "it: 139000, loss: 9.673795557984862e-08\n",
      "eep mind how are you doing bold fuck!rt deep mind how are yo\n",
      "it: 140000, loss: 1.0530524806274533e-07\n",
      "it: 141000, loss: 6.445673660012906e-08\n",
      "it: 142000, loss: 6.230464082201537e-08\n",
      "it: 143000, loss: 9.038378856533186e-08\n",
      "it: 144000, loss: 1.3478313665742334e-07\n",
      "it: 145000, loss: 1.1035598582841865e-07\n",
      "it: 146000, loss: 9.10091680226974e-08\n",
      "it: 147000, loss: 6.485695506927268e-08\n",
      "it: 148000, loss: 6.873130844896336e-08\n",
      "it: 149000, loss: 7.329586390382749e-08\n",
      "ind how are you doing bold fuck!re deep mind how are you doi\n",
      "it: 150000, loss: 8.940437875740144e-08\n",
      "it: 151000, loss: 9.741891886918737e-08\n",
      "it: 152000, loss: 5.966266355576298e-08\n",
      "it: 153000, loss: 5.7629402621600834e-08\n",
      "it: 154000, loss: 8.36132873555779e-08\n",
      "it: 155000, loss: 1.2492904079456112e-07\n",
      "it: 156000, loss: 1.0232439983961805e-07\n",
      "it: 157000, loss: 8.443815987533645e-08\n",
      "it: 158000, loss: 6.017851046861642e-08\n",
      "it: 159000, loss: 6.384124590491633e-08\n",
      "ow are you doing bold fuck!re deep mind how are you doing bo\n",
      "it: 160000, loss: 6.80959268983732e-08\n",
      "it: 161000, loss: 8.309684015852592e-08\n",
      "it: 162000, loss: 9.062546204771605e-08\n",
      "it: 163000, loss: 5.5528914216918116e-08\n",
      "it: 164000, loss: 5.359952670137319e-08\n",
      "it: 165000, loss: 7.777276986801522e-08\n",
      "it: 166000, loss: 1.16409878977378e-07\n",
      "it: 167000, loss: 9.537485760126478e-08\n",
      "it: 168000, loss: 7.874655037215281e-08\n",
      "it: 169000, loss: 5.612495249567876e-08\n",
      "e you doing bold fuck!rt deep mind how are you doing bold fu\n",
      "it: 170000, loss: 5.959727055678457e-08\n",
      "it: 171000, loss: 6.357991110007983e-08\n",
      "it: 172000, loss: 7.76145802591673e-08\n",
      "it: 173000, loss: 8.4712795462596e-08\n",
      "it: 174000, loss: 5.192807248411779e-08\n",
      "it: 175000, loss: 5.009049720124433e-08\n",
      "it: 176000, loss: 7.268374964692006e-08\n",
      "it: 177000, loss: 1.0897213204470764e-07\n",
      "it: 178000, loss: 8.930298931953824e-08\n",
      "it: 179000, loss: 7.376922417593301e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " doing bold fuck!re doep mind how are you doing bold fuck!rt\n",
      "it: 180000, loss: 5.25792464199203e-08\n",
      "it: 181000, loss: 5.5879501080795897e-08\n",
      "it: 182000, loss: 5.962151143274982e-08\n",
      "it: 183000, loss: 7.280594959897637e-08\n",
      "it: 184000, loss: 7.95203360868413e-08\n",
      "it: 185000, loss: 4.8763496802735365e-08\n",
      "it: 186000, loss: 4.700783763739764e-08\n",
      "it: 187000, loss: 6.821057755682711e-08\n",
      "it: 188000, loss: 1.0242259016770494e-07\n",
      "it: 189000, loss: 8.395278599715527e-08\n",
      "g bold fuck!at deep mind how are you doing bold fuck!rt deep\n",
      "it: 190000, loss: 6.937992731888226e-08\n",
      "it: 191000, loss: 4.945182687528491e-08\n",
      "it: 192000, loss: 5.259596085928617e-08\n",
      "it: 193000, loss: 5.612368723918216e-08\n",
      "it: 194000, loss: 6.855426728851195e-08\n",
      "it: 195000, loss: 7.492426971359805e-08\n",
      "it: 196000, loss: 4.596056252182153e-08\n",
      "it: 197000, loss: 4.427857679378064e-08\n",
      "it: 198000, loss: 6.42483323994625e-08\n",
      "it: 199000, loss: 9.661143548714996e-08\n",
      "d fuck!rt deep mind how are you doing bold fuck!rn deep mind\n",
      "it: 200000, loss: 7.920310426810797e-08\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-bb314f29dec1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mit\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'it: {}, loss: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mit\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-d852e2fc93da>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, grads)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparam_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdx2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparam_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparam_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam_value\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mm\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data = read_data()\n",
    "data = 'i will work at deep mind how are you doing bold fuck!'\n",
    "data_loader = DataLoader(data[:10000], batch_size=5)\n",
    "\n",
    "hidden_size, vocab_size = 100, data_loader.vocab_size\n",
    "rnn = RNN()\n",
    "params = init_params(vocab_size=vocab_size, hidden_size=hidden_size)\n",
    "optimizer = Adam(params, lr=1e-3)\n",
    "\n",
    "num_epochs = 100000\n",
    "\n",
    "it = 0\n",
    "for epoch in range(num_epochs):\n",
    "    hidden_state = np.zeros((1, hidden_size))\n",
    "    for x, y in data_loader:\n",
    "        if it % 10000 == 0:\n",
    "            one_hot = sample(rnn, hidden_state, x[0], params, n=60)\n",
    "            print(data_loader.decode(one_hot))\n",
    "        loss, hidden_state, dparams = rnn_training_step(rnn, hidden_state, x, y, params)\n",
    "        if it % 1000 == 0:\n",
    "            print('it: {}, loss: {}'.format(it, loss))\n",
    "        optimizer.step(dparams)\n",
    "        it += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
